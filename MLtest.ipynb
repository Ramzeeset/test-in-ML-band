{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = text.split() \n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_tfidf(array):\n",
    "    \n",
    "    def description_processing(line):\n",
    "        \n",
    "        a = ['.', '-', '(', ')', ',', ':', ';']\n",
    "        \n",
    "        for i in range(0, len(a)):\n",
    "        \n",
    "            line = line.replace(a[i], ' ')\n",
    "            \n",
    "        line = line.split()\n",
    "    \n",
    "        return line\n",
    "\n",
    "    def calculation_tf(text):\n",
    "        \n",
    "        tf_text = collections.Counter(text)\n",
    "    \n",
    "        for i in tf_text:\n",
    "        \n",
    "            tf_text[i] = tf_text[i]/float(len(text))\n",
    "        \n",
    "        return tf_text\n",
    "\n",
    "    def calculation_idf(word, array):\n",
    "        \n",
    "        n = 0\n",
    "        \n",
    "        for i in array:\n",
    "            \n",
    "            if word in i:\n",
    "                \n",
    "                n += 1\n",
    "                \n",
    "        return math.log10(len(array)/n)\n",
    "\n",
    "    documents_list = []\n",
    "\n",
    "    for line in array:\n",
    "\n",
    "        tf_idf_dictionary = {}\n",
    "\n",
    "        calculated_tf = calculation_tf(description_processing(line))\n",
    "\n",
    "        for word in calculated_tf:\n",
    "            \n",
    "            if (calculated_tf[word] * calculation_idf(word, array)) > 0.03:\n",
    "                \n",
    "                tf_idf_dictionary[word] = calculated_tf[word] * calculation_idf(word, array)\n",
    "            \n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "\n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train.csv')\n",
    "data = data_train['name'] + ' ' + data_train['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = calculation_tfidf(data)\n",
    "\n",
    "best = {'Менеджер': {},\n",
    "        'Искусство': {},\n",
    "        'Рабочий': {},\n",
    "        'Дизайнер': {},\n",
    "        'Специалист': {},\n",
    "        'СМИ': {},\n",
    "        'Врач': {},\n",
    "        'other': {},\n",
    "        'Инженер': {},\n",
    "        'IT': {},\n",
    "        'Право': {},\n",
    "        'Учитель': {},\n",
    "        'Агент': {}}\n",
    "\n",
    "for i in range(0, len(big)):\n",
    "    for key in big[i].keys():\n",
    "        key = lemmatize(key)[0]\n",
    "        if key not in best.get(data_train['category'][i]):\n",
    "            (best.get(data_train['category'][i])).update({key: 1})\n",
    "        else:\n",
    "            (best.get(data_train['category'][i])).update({key: (best.get(data_train['category'][i])).get(key) + 1})\n",
    "            \n",
    "best_list = []\n",
    "\n",
    "for big_key in best.keys():\n",
    "    a = []\n",
    "    for small_key in best[big_key].keys():\n",
    "        if (best[big_key]).get(small_key) >= 4:\n",
    "            a.append(small_key)\n",
    "            \n",
    "    if len(a) <= 20:\n",
    "        for small_key in best[big_key].keys():\n",
    "            if len(a) <= 20:\n",
    "                if (best[big_key]).get(small_key) >= 2:\n",
    "                    a.append(small_key)\n",
    "        \n",
    "\n",
    "                         \n",
    "    best_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('test.csv')\n",
    "data_test =  data_test['name'] + ' ' + data_test['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(0, len(data_test)):\n",
    "    a = 0\n",
    "    k = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for word in description_processing(data_test[i]):\n",
    "        \n",
    "        word = (morph.parse(word)[0]).normal_form\n",
    "            \n",
    "        for j in range(0, 13):\n",
    "            if word in best_list[j]:\n",
    "                k[j] = k[j] + 1\n",
    "    for j in range(0, 13):\n",
    "        if k[j] == np.max(k) and a == 0:\n",
    "            pred.append(pd.unique(data_train['category'])[j])\n",
    "            a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = []\n",
    "pred = np.array(pred)\n",
    "for i in range (0, 106):\n",
    "    Id.append(i)\n",
    "Id = np.reshape(Id, pred.shape)\n",
    "Answer = pd.DataFrame(Id)\n",
    "Answer.columns = ['id']\n",
    "Answer['category'] = pred\n",
    "Answer.to_csv(\"Answer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
